{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hdfs\n",
    "# ./hdfscli.cfg\n",
    "\n",
    "#     [global]\n",
    "#     default.alias = dev\n",
    "\n",
    "#     [dev.alias]\n",
    "#     url = http://localhost:9870\n",
    "\n",
    "# from hdfs import Config\n",
    "\n",
    "# client = Config().get_client('dev')\n",
    "# test = client.list('/test')\n",
    "# print(test)\n",
    "\n",
    "# with client.read('/test/movies.csv') as reader:\n",
    "#     movies = reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Pagerank with graph Projections\") \\\n",
    "        .config(\"spark.driver.memory\", \"6g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "spark.sparkContext._conf.getAll()  # check the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = spark.read.csv('hdfs://localhost:9001/test/ratings.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = spark.read.csv('hdfs://localhost:9001/test/movies.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_matrix = ratings.join(movies, on=\"movieId\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_score = {\n",
    "    0.5: -1.0,\n",
    "    1: -1.0,\n",
    "    1.5: -0.5,\n",
    "    2: 0.0,\n",
    "    2.5: 0.0,\n",
    "    3: 0.0,\n",
    "    3.5: 0.5,\n",
    "    4: 1.0,\n",
    "    4.5: 1.1,\n",
    "    5: 1.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "map_score_udf = spark.udf.register(\"map_score\", lambda x: mapping_score.get(x, 0), FloatType())\n",
    "user_movie_matrix = user_movie_matrix.withColumn(\"weight\", map_score_udf(col(\"rating\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = user_movie_matrix.select(\n",
    "    col(\"userId\").cast(StringType()).alias(\"src\"),\n",
    "    col(\"title\").alias(\"dst\"),\n",
    "    col(\"weight\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "user_vertices = user_movie_matrix.select(col(\"userId\").cast(StringType()).alias(\"id\")).distinct().withColumn(\"bipartite\", lit(0))\n",
    "movie_vertices = user_movie_matrix.select(col(\"title\").alias(\"id\")).distinct().withColumn(\"bipartite\", lit(1))\n",
    "vertices = user_vertices.union(movie_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39261370/unable-to-run-a-basic-graphframes-example\n",
    "\n",
    "# Depending on your spark version, all you have to do is download the graphframe jar corresponding to your version\n",
    "# of spark here https://spark-packages.org/package/graphframes/graphframes.\n",
    "\n",
    "# Then you'll have to copy the jar downloaded to your spark jar directory and rename it to graphframes.jar.\n",
    "\n",
    "# # # pyspark --packages graphframes:graphframes-0.8.4-spark3.5-s_2.12 --jars graphframes-0.8.4-spark3.5-s_2.12.jar\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "user_movie_graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check the graph construction\n",
    "print(\"First 10 nodes in the graph:\")\n",
    "user_movie_graph.vertices.show(10, truncate=False)\n",
    "print(\"First 10 edges in the graph:\")\n",
    "user_movie_graph.edges.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_user_edges = user_movie_graph.edges.alias(\"e1\") \\\n",
    "    .join(user_movie_graph.edges.alias(\"e2\"), col(\"e1.dst\") == col(\"e2.dst\")) \\\n",
    "    .select(\n",
    "        col(\"e1.src\").alias(\"src\"),\n",
    "        col(\"e2.src\").alias(\"dst\"),\n",
    "        (col(\"e1.weight\") + col(\"e2.weight\")).alias(\"weight\")\n",
    "    ).filter(col(\"src\") != col(\"dst\"))\n",
    "\n",
    "user_user_graph = GraphFrame(user_movie_graph.vertices, user_user_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check the graph construction\n",
    "print(\"First 10 nodes in the graph:\")\n",
    "user_user_graph.vertices.show(10, truncate=False)\n",
    "print(\"First 10 edges in the graph:\")\n",
    "user_user_graph.edges.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project movie-movie graph\n",
    "movie_movie_edges = user_movie_graph.edges.alias(\"e1\") \\\n",
    "    .join(user_movie_graph.edges.alias(\"e2\"), col(\"e1.src\") == col(\"e2.src\")) \\\n",
    "    .select(\n",
    "        col(\"e1.dst\").alias(\"src\"),\n",
    "        col(\"e2.dst\").alias(\"dst\"),\n",
    "        (col(\"e1.weight\") + col(\"e2.weight\")).alias(\"weight\")\n",
    "    ).filter(col(\"src\") != col(\"dst\"))\n",
    "\n",
    "movie_movie_graph = GraphFrame(user_movie_graph.vertices, movie_movie_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check the graph construction\n",
    "print(\"First 10 nodes in the graph:\")\n",
    "movie_movie_graph.vertices.show(10, truncate=False)\n",
    "print(\"First 10 edges in the graph:\")\n",
    "movie_movie_graph.edges.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preference_vector(user_id, user_movie_graph):\n",
    "    edges = user_movie_graph.edges.filter(col(\"src\") == user_id).rdd.map(lambda row: (row[\"dst\"], row[\"weight\"])).collect()\n",
    "    print(f\"Preference vector for user {user_id}: {list(edges)[:10]}\")\n",
    "    \n",
    "    #\n",
    "    tot = sum([weight for _, weight in edges])\n",
    "    #\n",
    "    \n",
    "    print(f\"Total weight for user {user_id}: {tot}\")\n",
    "    if tot > 0:\n",
    "        return {movie: weight / tot for movie, weight in edges}\n",
    "    else:\n",
    "        movies = user_movie_graph.vertices.filter(col(\"bipartite\") == 1).select(\"id\").rdd.map(lambda row: row[0]).collect()\n",
    "        return {movie: 1 / len(movies) for movie in movies}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user(user_id, user_movie_graph, movie_movie_graph):\n",
    "    p_vec = create_preference_vector(user_id, user_movie_graph)\n",
    "    print(f\"Preference vector for user {user_id}: {list(p_vec)[:10]}\")\n",
    "    already_seen = [movie for movie, weight in p_vec.items() if weight > 0]\n",
    "    print(f\"Already seen movies for user {user_id}: {list(already_seen)[:10]}\")\n",
    "    if len(already_seen) == len(p_vec):\n",
    "        return []\n",
    "    pagerank_results = movie_movie_graph.pageRank(resetProbability=0.95, maxIter=20)\n",
    "    item_rank = pagerank_results.vertices.select(\"id\", \"pagerank\").rdd.map(lambda row: (row[\"id\"], row[\"pagerank\"])).collectAsMap()\n",
    "    recommendations = sorted(\n",
    "        (movie for movie in item_rank if movie not in already_seen),\n",
    "        key=lambda x: item_rank[x], reverse=True\n",
    "    )\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"10\"\n",
    "s_t = predict_user(user, user_movie_graph, movie_movie_graph)\n",
    "print(f\"Predicted movies for user {user}: {s_t[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
