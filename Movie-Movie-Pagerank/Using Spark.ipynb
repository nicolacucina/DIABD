{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Python 3.11.9 for dependencies problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for hdfs usage, not used\n",
    "\n",
    "# pip install hdfs\n",
    "# ./hdfscli.cfg\n",
    "\n",
    "#     [global]\n",
    "#     default.alias = dev\n",
    "\n",
    "#     [dev.alias]\n",
    "#     url = http://localhost:9870\n",
    "\n",
    "# from hdfs import Config\n",
    "\n",
    "# client = Config().get_client('dev')\n",
    "# test = client.list('/test')\n",
    "# print(test)\n",
    "\n",
    "# with client.read('/test/movies.csv') as reader:\n",
    "#     movies = reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Pagerank with graph Projections\") \\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"spark.default.parallelism\", \"12\") \\\n",
    "        .config(\"spark.executor.memory\", \"6g\") \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.hadoop.fs.DefaultFS\", \"hdfs://localhost:50054\") \\\n",
    "        .config(\"spark.memory.storageFraction\" , \"0.3\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "        .config(\"spark.locality.wait.node\", \"0\") \\\n",
    "        .getOrCreate()\n",
    "# .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the spark session\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?\n",
    "\n",
    "# from pyspark.sql import SQLContext\n",
    "# sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext._conf.getAll()  # check the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_port = 50054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = spark.read.csv('hdfs://localhost:'+ str(hdfs_port) +'/test/ratings.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = spark.read.csv('hdfs://localhost:'+ str(hdfs_port) +'/test/movies.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_matrix = ratings.join(movies, on=\"movieId\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_score = {\n",
    "    0.5: -1.0,\n",
    "    1: -1.0,\n",
    "    1.5: -0.5,\n",
    "    2: 0.0,\n",
    "    2.5: 0.0,\n",
    "    3: 0.0,\n",
    "    3.5: 0.5,\n",
    "    4: 1.0,\n",
    "    4.5: 1.1,\n",
    "    5: 1.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# mapping_udf = udf(lambda x: mapping_score.get(x, 0))\n",
    "# ratings = ratings.withColumn(\"mapped_rating\", mapping_udf(col(\"rating\")))\n",
    "\n",
    "\n",
    "map_score_udf = spark.udf.register(\"map_score\", lambda x: mapping_score.get(x, 0), FloatType())\n",
    "user_movie_matrix = user_movie_matrix.withColumn(\"weight\", map_score_udf(col(\"rating\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_movie_edges = ratings.select(\"userId\", \"title\", \"mapped_rating\")\n",
    "\n",
    "edges = user_movie_matrix.select(\n",
    "    col(\"userId\").cast(StringType()).alias(\"src\"), # <- why string?\n",
    "    col(\"title\").alias(\"dst\"),\n",
    "    col(\"weight\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "user_vertices = user_movie_matrix.select(col(\"userId\").cast(StringType()).alias(\"id\")).distinct().withColumn(\"bipartite\", lit(0))\n",
    "movie_vertices = user_movie_matrix.select(col(\"title\").alias(\"id\")).distinct().withColumn(\"bipartite\", lit(1))\n",
    "vertices = user_vertices.union(movie_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39261370/unable-to-run-a-basic-graphframes-example\n",
    "\n",
    "# Depending on your spark version, all you have to do is download the graphframe jar corresponding to your version\n",
    "# of spark here https://spark-packages.org/package/graphframes/graphframes.\n",
    "\n",
    "# Then you'll have to copy the jar downloaded to your spark jar directory and rename it to graphframes.jar.\n",
    "\n",
    "# # # pyspark --packages graphframes:graphframes:0.8.4-spark3.5-s_2.12\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "user_movie_graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively find the size of objects.\"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Mark as seen\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size\n",
    "\n",
    "# Assuming user_movie_graph is your object\n",
    "memory_consumption = get_size(user_movie_graph)\n",
    "print(f\"Memory consumption of user_movie_graph: {memory_consumption} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GraphX ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check the graph construction\n",
    "print(\"First 10 nodes in the graph:\")\n",
    "user_movie_graph.vertices.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 10 edges in the graph:\")\n",
    "user_movie_graph.edges.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project movie-movie graph\n",
    "movie_movie_edges = user_movie_graph.edges.alias(\"e1\") \\\n",
    "    .join(user_movie_graph.edges.alias(\"e2\"), col(\"e1.src\") == col(\"e2.src\")) \\\n",
    "    .select(\n",
    "        col(\"e1.dst\").alias(\"src\"),\n",
    "        col(\"e2.dst\").alias(\"dst\"),\n",
    "        (col(\"e1.weight\") + col(\"e2.weight\")).alias(\"weight\")\n",
    "    ).filter(col(\"src\") != col(\"dst\"))\n",
    "\n",
    "movie_movie_graph = GraphFrame(user_movie_graph.vertices, movie_movie_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check the graph construction\n",
    "print(\"First 10 nodes in the graph:\")\n",
    "movie_movie_graph.vertices.show(10, truncate=False)\n",
    "print(\"First 10 edges in the graph:\")\n",
    "movie_movie_graph.edges.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preference_vector(user_id, user_movie_graph):\n",
    "    edges = user_movie_graph.edges.filter(col(\"src\") == user_id).rdd.map(lambda row: (row[\"dst\"], row[\"weight\"])).collect()\n",
    "    print(f\"Preference vector for user {user_id}: {list(edges)[:10]}\")\n",
    "    \n",
    "    #\n",
    "    tot = sum([weight for _, weight in edges])\n",
    "    #\n",
    "    \n",
    "    print(f\"Total weight for user {user_id}: {tot}\")\n",
    "    if tot > 0:\n",
    "        return {movie: weight / tot for movie, weight in edges} # <- qua è diverso perchè assegno peso SOLO a quelli visti, in python normale assegnavmo a tutti i film qualcosa\n",
    "    else:\n",
    "        movies = user_movie_graph.vertices.filter(col(\"bipartite\") == 1).select(\"id\").rdd.map(lambda row: row[0]).collect()\n",
    "        return {movie: 1 / len(movies) for movie in movies}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user(user_id, user_movie_graph, movie_movie_graph):\n",
    "    p_vec = create_preference_vector(user_id, user_movie_graph)\n",
    "    print(f\"Preference vector for user {user_id}: {list(p_vec)[:10]}\")\n",
    "    already_seen = [movie for movie, weight in p_vec.items() if weight > 0]\n",
    "    print(f\"Already seen movies for user {user_id}: {list(already_seen)[:10]}\")\n",
    "    if len(already_seen) == len(p_vec):\n",
    "        return []\n",
    "    pagerank_results = movie_movie_graph.pageRank(resetProbability=0.95, maxIter=20, sourceId=p_vec.keys(), sourceWeight=p_vec.values()) # esiste sourceWeight? non sembra\n",
    "    item_rank = pagerank_results.vertices.select(\"id\", \"pagerank\").rdd.map(lambda row: (row[\"id\"], row[\"pagerank\"])).collectAsMap()\n",
    "    recommendations = sorted(\n",
    "        (movie for movie in item_rank if movie not in already_seen),\n",
    "        key=lambda x: item_rank[x], reverse=True\n",
    "    )\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"10\"\n",
    "s_t = predict_user(user, user_movie_graph, movie_movie_graph)\n",
    "print(f\"Predicted movies for user {user}: {s_t[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
