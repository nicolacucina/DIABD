{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python 3.11.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/master/spark-3.5.4-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/master/.ivy2/cache\n",
      "The jars for the packages stored in: /home/master/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f212c871-194f-4dae-b9dc-cc256ea72fc8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.4-spark3.5-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 217ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.4-spark3.5-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f212c871-194f-4dae-b9dc-cc256ea72fc8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/10ms)\n",
      "25/01/08 14:29:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/08 14:29:12 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/08 14:29:19 WARN Client: Same path resource file:///home/master/.ivy2/jars/graphframes_graphframes-0.8.4-spark3.5-s_2.12.jar added multiple times to distributed cache.\n",
      "25/01/08 14:29:19 WARN Client: Same path resource file:///home/master/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Pagerank with graph Projections\") \\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"spark.executor.instances\", \"3\") \\\n",
    "        .config(\"spark.executor.memory\", \"6g\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.locality.wait.node\", \"0\") \\\n",
    "        .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "        .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.4-spark3.5-s_2.12\") \\\n",
    "        .getOrCreate()\n",
    "# .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
    "# .config(\"spark.hadoop.fs.DefaultFS\", \"hdfs://localhost:9000\") \\\n",
    "#        .config(\"spark.memory.storageFraction\" , \"0.3\") \\\n",
    "#        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "# Test the spark session\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.yarn.secondary.jars',\n",
       "  'graphframes_graphframes-0.8.4-spark3.5-s_2.12.jar,org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.executor.instances', '3'),\n",
       " ('spark.driver.appUIAddress', 'http://master:4040'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://master:8088/proxy/application_1736335349974_0003'),\n",
       " ('spark.app.startTime', '1736342949260'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'master'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9.7-src.zip<CPS>{{PWD}}/graphframes_graphframes-0.8.4-spark3.5-s_2.12.jar<CPS>{{PWD}}/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.locality.wait.node', '0'),\n",
       " ('spark.app.submitTime', '1736342948942'),\n",
       " ('spark.yarn.dist.pyFiles',\n",
       "  'file:///home/master/.ivy2/jars/graphframes_graphframes-0.8.4-spark3.5-s_2.12.jar,file:///home/master/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.jars.packages', 'graphframes:graphframes:0.8.4-spark3.5-s_2.12'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///home/master/.ivy2/jars/graphframes_graphframes-0.8.4-spark3.5-s_2.12.jar,file:///home/master/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/home/master/.ivy2/jars/graphframes_graphframes-0.8.4-spark3.5-s_2.12.jar,/home/master/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.driver.host', 'master'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.yarn.dist.jars',\n",
       "  'file:///home/master/.ivy2/jars/graphframes_graphframes-0.8.4-spark3.5-s_2.12.jar,file:///home/master/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar'),\n",
       " ('spark.app.id', 'application_1736335349974_0003'),\n",
       " ('spark.executor.memory', '6g'),\n",
       " ('spark.executor.memoryOverhead', '2g'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.name', 'Pagerank with graph Projections'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.driver.port', '37173'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._conf.getAll()  # check the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_port = 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "ratings = spark.read.csv('hdfs://master:'+ str(hdfs_port) +'/dataset/ratings.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = spark.read.csv('hdfs://master:'+ str(hdfs_port) +'/dataset/movies.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_matrix = ratings.join(movies, on=\"movieId\", how=\"inner\") # parallizzare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "      <td>Heat (1995)</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "      <td>Seven (a.k.a. Se7en) (1995)</td>\n",
       "      <td>Mystery|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "      <td>Usual Suspects, The (1995)</td>\n",
       "      <td>Crime|Mystery|Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId  userId  rating  timestamp                        title  \\\n",
       "0        1       1     4.0  964982703             Toy Story (1995)   \n",
       "1        3       1     4.0  964981247      Grumpier Old Men (1995)   \n",
       "2        6       1     4.0  964982224                  Heat (1995)   \n",
       "3       47       1     5.0  964983815  Seven (a.k.a. Se7en) (1995)   \n",
       "4       50       1     5.0  964982931   Usual Suspects, The (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                               Comedy|Romance  \n",
       "2                        Action|Crime|Thriller  \n",
       "3                             Mystery|Thriller  \n",
       "4                       Crime|Mystery|Thriller  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_movie_matrix.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_score = {\n",
    "    0.5: -1.2,\n",
    "    1: -1.1,\n",
    "    1.5: -1.0,\n",
    "    2: -0.5,\n",
    "    2.5: -0.1,\n",
    "    3: 0.1,\n",
    "    3.5: 0.5,\n",
    "    4: 1.0,\n",
    "    4.5: 1.1,\n",
    "    5: 1.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, FloatType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "map_score_udf = spark.udf.register(\"map_score\", lambda x: mapping_score.get(x, 0), FloatType())\n",
    "user_movie_matrix = user_movie_matrix.withColumn(\"weight\", map_score_udf(col(\"rating\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = user_movie_matrix.select(\n",
    "    col(\"userId\").cast(StringType()).alias(\"src\"),\n",
    "    col(\"title\").alias(\"dst\"),\n",
    "    col(\"weight\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "user_vertices = user_movie_matrix.select(col(\"userId\").cast(StringType()).alias(\"id\")).distinct().withColumn(\"bipartite\", lit(0))\n",
    "movie_vertices = user_movie_matrix.select(col(\"title\").alias(\"id\")).distinct().withColumn(\"bipartite\", lit(1))\n",
    "vertices = user_vertices.union(movie_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install graphframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Stackoverflow](https://stackoverflow.com/questions/39261370/unable-to-run-a-basic-graphframes-example)\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "Depending on your spark version, all you have to do is download the graphframe jar corresponding to your version\n",
    "of spark here https://spark-packages.org/package/graphframes/graphframes.\n",
    "\n",
    "Then you'll have to copy the jar downloaded to your spark jar directory and run\n",
    "\n",
    "```shell\n",
    "pyspark --packages graphframes:graphframes-0.8.4-spark3.5-s_2.12 --jars graphframes-0.8.4-spark3.5-s_2.12.jar\n",
    "```\n",
    "\n",
    "This is included in the SparkConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "user_movie_graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 nodes in the graph:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|id |bipartite|\n",
      "+---+---------+\n",
      "|296|0        |\n",
      "|467|0        |\n",
      "|125|0        |\n",
      "|451|0        |\n",
      "|7  |0        |\n",
      "|51 |0        |\n",
      "|124|0        |\n",
      "|447|0        |\n",
      "|591|0        |\n",
      "|307|0        |\n",
      "+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"First 10 nodes in the graph:\")\n",
    "user_movie_graph.vertices.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di nodi con bipartite = 0 (utenti): 610\n",
      "Esempio di nodi con bipartite = 0 (utenti):\n",
      "+---+---------+\n",
      "|id |bipartite|\n",
      "+---+---------+\n",
      "|296|0        |\n",
      "|467|0        |\n",
      "|125|0        |\n",
      "|451|0        |\n",
      "|7  |0        |\n",
      "|51 |0        |\n",
      "|124|0        |\n",
      "|447|0        |\n",
      "|591|0        |\n",
      "|307|0        |\n",
      "+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_nodes = user_movie_graph.vertices.filter(col(\"bipartite\") == 0)\n",
    "user_count = user_nodes.count()\n",
    "print(f\"Numero di nodi con bipartite = 0 (utenti): {user_count}\")\n",
    "user_nodes.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di nodi con bipartite = 1 (film): 9719\n",
      "Esempio di nodi con bipartite = 1 (film):\n",
      "+--------------------------------------+---------+\n",
      "|id                                    |bipartite|\n",
      "+--------------------------------------+---------+\n",
      "|Psycho (1960)                         |1        |\n",
      "|Men in Black (a.k.a. MIB) (1997)      |1        |\n",
      "|Gulliver's Travels (1939)             |1        |\n",
      "|Heavenly Creatures (1994)             |1        |\n",
      "|Elizabeth (1998)                      |1        |\n",
      "|Before Night Falls (2000)             |1        |\n",
      "|O Brother, Where Art Thou? (2000)     |1        |\n",
      "|Snow White and the Seven Dwarfs (1937)|1        |\n",
      "|Three Wishes (1995)                   |1        |\n",
      "|When We Were Kings (1996)             |1        |\n",
      "+--------------------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_nodes = user_movie_graph.vertices.filter(col(\"bipartite\") == 1)\n",
    "movie_count = movie_nodes.count()\n",
    "print(f\"Numero di nodi con bipartite = 1 (film): {movie_count}\")\n",
    "movie_nodes.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 edges in the graph:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------+------+\n",
      "|src|dst                        |weight|\n",
      "+---+---------------------------+------+\n",
      "|1  |Toy Story (1995)           |1.0   |\n",
      "|1  |Grumpier Old Men (1995)    |1.0   |\n",
      "|1  |Heat (1995)                |1.0   |\n",
      "|1  |Seven (a.k.a. Se7en) (1995)|1.2   |\n",
      "|1  |Usual Suspects, The (1995) |1.2   |\n",
      "|1  |From Dusk Till Dawn (1996) |0.1   |\n",
      "|1  |Bottle Rocket (1996)       |1.2   |\n",
      "|1  |Braveheart (1995)          |1.0   |\n",
      "|1  |Rob Roy (1995)             |1.2   |\n",
      "|1  |Canadian Bacon (1995)      |1.2   |\n",
      "+---+---------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "num_edges = user_movie_graph.edges.count()\n",
    "print(f\"Number of edges: {num_edges}\")\n",
    "\n",
    "print(\"First 10 edges in the graph:\")\n",
    "user_movie_graph.edges.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory usage for vertices: 0.46789073944091797 MB\n",
      "Estimated memory usage for edges: 6.4138898849487305 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Check the memory occupation\n",
    "vertices_size = user_movie_graph.vertices.rdd.map(lambda x: len(str(x))).sum()\n",
    "edges_size = user_movie_graph.edges.rdd.map(lambda x: len(str(x))).sum()\n",
    "\n",
    "print(f\"Estimated memory usage for vertices: {vertices_size/(1024**2)} MB\")\n",
    "print(f\"Estimated memory usage for edges: {edges_size/(1024**2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_nodes = user_movie_graph.vertices.filter(col(\"bipartite\") == 1).select(\"id\").alias(\"movies\")\n",
    "print(f\"Number of nodes{movie_nodes.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python the [projection algoritm](https://networkx.org/documentation/stable/_modules/networkx/algorithms/bipartite/projection.html#weighted_projected_graph) is implemented as follows:\n",
    "\n",
    "```python\n",
    "def weighted_projected_graph(B, nodes, ratio=False):\n",
    "    \n",
    "    if B.is_directed():\n",
    "        pred = B.pred\n",
    "        G = nx.DiGraph()\n",
    "    else:\n",
    "        pred = B.adj\n",
    "        G = nx.Graph()\n",
    "    G.graph.update(B.graph)\n",
    "    G.add_nodes_from((n, B.nodes[n]) for n in nodes)\n",
    "    n_top = len(B) - len(nodes)\n",
    "\n",
    "    if n_top < 1:\n",
    "        raise NetworkXAlgorithmError(\n",
    "            f\"the size of the nodes to project onto ({len(nodes)}) is >= the graph size ({len(B)}).\\n\"\n",
    "            \"They are either not a valid bipartite partition or contain duplicates\"\n",
    "        )\n",
    "\n",
    "    for u in nodes:\n",
    "        unbrs = set(B[u])\n",
    "        nbrs2 = {n for nbr in unbrs for n in B[nbr]} - {u}\n",
    "        for v in nbrs2:\n",
    "            vnbrs = set(pred[v])\n",
    "            common = unbrs & vnbrs\n",
    "            if not ratio:\n",
    "                weight = len(common)\n",
    "            else:\n",
    "                weight = len(common) / n_top\n",
    "            G.add_edge(u, v, weight=weight)\n",
    "    return G\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, size, collect_set, array_intersect\n",
    "\n",
    "def weighted_projected_graph(edges, movies, ratio=False):\n",
    "    movie_neighbors = edges.groupBy(\"src\").agg(collect_set(\"dst\").alias(\"neighbors\"))\n",
    "    movie_neighbors = movie_neighbors.alias(\"mn1\").join(movie_neighbors.alias(\"mn2\"), col(\"mn1.src\") < col(\"mn2.src\")) \\\n",
    "        .select(\n",
    "            col(\"mn1.src\").alias(\"movie1\"),\n",
    "            col(\"mn2.src\").alias(\"movie2\"),\n",
    "            array_intersect(col(\"mn1.neighbors\"), col(\"mn2.neighbors\")).alias(\"shared_neighbors\")\n",
    "        )\n",
    "\n",
    "    if not ratio:\n",
    "        movie_neighbors = movie_neighbors.withColumn(\"weight\", size(col(\"shared_neighbors\")))\n",
    "    else:\n",
    "        n_top = edges.select(\"src\").distinct().count()\n",
    "        movie_neighbors = movie_neighbors.withColumn(\"weight\", size(col(\"shared_neighbors\")) / n_top)\n",
    "    \n",
    "    movie_neighbors = movie_neighbors.filter(col(\"weight\") > 0)\n",
    "\n",
    "    movie_movie_edges = movie_neighbors.select(\n",
    "        col(\"movie1\").alias(\"src\"),\n",
    "        col(\"movie2\").alias(\"dst\"),\n",
    "        col(\"weight\")\n",
    "    )\n",
    "    \n",
    "    movie_movie_graph = GraphFrame(movies, movie_movie_edges)\n",
    "    return movie_movie_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vertices (nodes): 9719\n",
      "Number of edges: 164054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "movie_movie_graph = weighted_projected_graph(user_movie_graph.edges, movie_nodes, ratio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vertices = movie_movie_graph.vertices.count()\n",
    "num_edges = movie_movie_graph.edges.count()\n",
    "print(f\"Number of vertices (nodes): {num_vertices}\")\n",
    "print(f\"Number of edges: {num_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 edges in the graph:\n",
      "+--------------------------------------+\n",
      "|id                                    |\n",
      "+--------------------------------------+\n",
      "|Psycho (1960)                         |\n",
      "|Men in Black (a.k.a. MIB) (1997)      |\n",
      "|Gulliver's Travels (1939)             |\n",
      "|Heavenly Creatures (1994)             |\n",
      "|Elizabeth (1998)                      |\n",
      "|Before Night Falls (2000)             |\n",
      "|O Brother, Where Art Thou? (2000)     |\n",
      "|Snow White and the Seven Dwarfs (1937)|\n",
      "|Three Wishes (1995)                   |\n",
      "|When We Were Kings (1996)             |\n",
      "+--------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 edges in the graph:\")\n",
    "movie_movie_graph.vertices.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 edges in the graph:\n",
      "+---+---+------+\n",
      "|src|dst|weight|\n",
      "+---+---+------+\n",
      "|100|101|7     |\n",
      "|100|102|12    |\n",
      "|100|104|35    |\n",
      "|100|107|7     |\n",
      "|100|11 |10    |\n",
      "|100|110|3     |\n",
      "|100|111|22    |\n",
      "|100|112|14    |\n",
      "|100|113|23    |\n",
      "|100|118|7     |\n",
      "+---+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 edges in the graph:\")\n",
    "movie_movie_graph.edges.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory usage for vertices: 0.33237457275390625 MB\n",
      "Estimated memory usage for edges: 5.493730545043945 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Check the memory occupation\n",
    "vertices_size = movie_movie_graph.vertices.rdd.map(lambda x: len(str(x))).sum()\n",
    "edges_size = movie_movie_graph.edges.rdd.map(lambda x: len(str(x))).sum()\n",
    "\n",
    "print(f\"Estimated memory usage for vertices: {vertices_size/(1024**2)} MB\")\n",
    "print(f\"Estimated memory usage for edges: {edges_size/(1024**2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preference_vector(user_id, user_movie_graph):\n",
    "    edges = user_movie_graph.edges.filter(col(\"src\") == user_id).rdd.map(lambda row: (row[\"dst\"], row[\"weight\"])).collect()\n",
    "    print(f\"Preference vector for user {user_id}: {list(edges)[:10]}\")\n",
    "    \n",
    "    tot = sum([weight for _, weight in edges])\n",
    "    \n",
    "    print(f\"Total weight for user {user_id}: {tot}\")\n",
    "    if tot > 0:\n",
    "        return {movie: weight / tot for movie, weight in edges} # <- qua è diverso perchè assegno peso SOLO a quelli visti, in python normale assegnavmo a tutti i film qualcosa\n",
    "    else:\n",
    "        movies = user_movie_graph.vertices.filter(col(\"bipartite\") == 1).select(\"id\").rdd.map(lambda row: row[0]).collect()\n",
    "        return {movie: 1 / len(movies) for movie in movies}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference vector for user 10: [('Pulp Fiction (1994)', -1.100000023841858), ('Forrest Gump (1994)', 0.5), ('Aladdin (1992)', 1.0), ('Pretty Woman (1990)', 0.5), ('Casablanca (1942)', 1.0), ('Mary Poppins (1964)', -1.2000000476837158), ('Dirty Dancing (1987)', 0.10000000149011612), ('Graduate, The (1967)', 0.10000000149011612), ('When Harry Met Sally... (1989)', 0.10000000149011612), ('As Good as It Gets (1997)', 0.5)]\n",
      "Total weight for user 10: 50.10000038892031\n",
      "The user has rated 140 movies\n"
     ]
    }
   ],
   "source": [
    "user = \"10\"\n",
    "p_vec = create_preference_vector(user, user_movie_graph)\n",
    "print(\"The user has rated\", len(p_vec), \"movies\")\n",
    "already_seen = [movie for movie, weight in p_vec.items() if weight > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user(user_id, user_movie_graph, movie_movie_graph):\n",
    "    p_vec = create_preference_vector(user_id, user_movie_graph)\n",
    "    print(f\"Preference vector for user {user_id}: {list(p_vec)[:10]}\")\n",
    "    already_seen = [movie for movie, weight in p_vec.items() if weight > 0]\n",
    "    print(f\"Already seen movies for user {user_id}: {list(already_seen)[:10]}\")\n",
    "    if len(already_seen) == len(p_vec):\n",
    "        return []\n",
    "\n",
    "    temp =  []\n",
    "    for k, v in p_vec.items():\n",
    "        temp.append(k)\n",
    "\n",
    "    pagerank_results = movie_movie_graph.pageRank(resetProbability=0.95, maxIter=20)\n",
    "    #pagerank_results = movie_movie_graph.pageRank(resetProbability=0.95, maxIter=20, sourceId=p_vec.keys())\n",
    "    #pagerank_results = movie_movie_graph.parallelPersonalizedPageRank(resetProbability=0.15, maxIter=10, sourceIds=temp)\n",
    "    \n",
    "    item_rank = pagerank_results.vertices.select(\"id\", \"pageranks\").rdd.map(lambda row: (row[\"id\"], row[\"pageranks\"])).collectAsMap()\n",
    "    \n",
    "    #\n",
    "    recommendations = sorted(\n",
    "        (movie for movie in item_rank if movie not in already_seen),\n",
    "        key=lambda x: item_rank[x], reverse=True\n",
    "    )\n",
    "    #\n",
    "\n",
    "    return recommendations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user = \"10\"\n",
    "s_t = predict_user(user, user_movie_graph, movie_movie_graph)\n",
    "print(f\"Predicted movies for user {user}: {s_t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete old test\n",
    "if upload_test:\n",
    "    def delete_test(tx, userId, movieId):\n",
    "        tx.run(\"MATCH (u:User {userId: $userId})-[r:RECOMMENDED]->(m:Movie {movieId: $movieId}) DELETE r\", userId=userId, movieId=movieId)\n",
    "\n",
    "    with driver.session() as session:\n",
    "        for movieId in s_t:           \n",
    "            session.execute_write(delete_test, user, movieId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the test\n",
    "if upload_test:\n",
    "    def create_recommendations(tx, userId, recs):\n",
    "            for rec in recs:\n",
    "                tx.run(\"MATCH (u:User {userId: $userId}), (m:Movie {title: $title})\"\n",
    "                    \"MERGE (u)-[:RECOMMENDED]->(m)\",\n",
    "                    userId=userId, title=rec)\n",
    "\n",
    "    with driver.session() as session:\n",
    "        session.execute_write(create_recommendations, user, s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recommendations(tx, userId, recs):\n",
    "    for rec in recs:\n",
    "        tx.run(\"MATCH (u:User {userId: $userId}), (m:Movie {title: $title})\"\n",
    "            \"MERGE (u)-[:RECOMMENDED]->(m)\",\n",
    "            userId=userId, title=rec)\n",
    "            \n",
    "if upload_predictions:\n",
    "    with driver.session() as session:\n",
    "       for user in filter_nodes(user_movie_graph, 0):\n",
    "           recs = predict_user(False, user, user_movie_graph, movie_movie_graph)    \n",
    "           session.execute_write(create_recommendations, user, recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
