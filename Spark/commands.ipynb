{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "#per avviare il Master Spark da terminale:\n",
    "spark-class org.apache.spark.deploy.master.Master\n",
    "\n",
    "#la pagina web del Master Spark si trova all'indirizzo: http://localhost:8080\n",
    "\n",
    "#per avviare il Worker Spark da terminale:\n",
    "spark-class org.apache.spark.deploy.worker.Worker spark://hostname:7077\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEORIA:\n",
    "- Q: What does Stage Skipped mean inSpark?\n",
    "- A: Typically it means that data has been fetched from cache and there was no need to re-execute given stage. It is consistent with your DAG which shows that the next stage requires shuffling (reduceByKey). Whenever there is shuffling involved Spark automatically caches generated data:\n",
    "Shuffle also generates a large number of intermediate files on disk. As of Spark 1.3, these files are preserved until the corresponding RDDs are no longer used and are garbage collected. This is done so the shuffle files donâ€™t need to be re-created if the lineage is re-computed.\n",
    "\n",
    "from: https://stackoverflow.com/questions/34580662/what-does-stage-skipped-mean-in-apache-spark-web-ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/master/spark-3.5.4-bin-hadoop3/')\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "        # .config(\"spark.default.parallelism\", \"12\") \\\n",
    "        # .config(\"spark.memory.storageFraction\" , \"0.3\") \\\n",
    "        # .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "# .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for hdfs usage, not used\n",
    "\n",
    "# pip install hdfs\n",
    "# ./hdfscli.cfg\n",
    "\n",
    "#     [global]\n",
    "#     default.alias = dev\n",
    "\n",
    "#     [dev.alias]\n",
    "#     url = http://localhost:9870\n",
    "\n",
    "# from hdfs import Config\n",
    "\n",
    "# client = Config().get_client('dev')\n",
    "# test = client.list('/test')\n",
    "# print(test)\n",
    "\n",
    "# with client.read('/test/movies.csv') as reader:\n",
    "#     movies = reader.read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
