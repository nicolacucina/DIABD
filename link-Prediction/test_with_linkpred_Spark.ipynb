{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USARE LINK PREDICTION CON ADAMIC ADAR PER PAGE RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hdfs\n",
    "# ./hdfscli.cfg\n",
    "\n",
    "#     [global]\n",
    "#     default.alias = dev\n",
    "\n",
    "#     [dev.alias]\n",
    "#     url = http://localhost:9870\n",
    "\n",
    "# from hdfs import Config\n",
    "\n",
    "# client = Config().get_client('dev')\n",
    "# test = client.list('/test')\n",
    "# print(test)\n",
    "\n",
    "# with client.read('/test/movies.csv') as reader:\n",
    "#     movies = reader.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.3\n",
      "Connected to master: spark://192.168.0.112:7077\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "Connection successful, Spark is ready!\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# Creazione della SparkSession\n",
    "# spark = SparkSession.builder.appName(\"Spark GraphFrames Example\").getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Configura SparkSession per connettersi al master\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Spark GraphFrames Example\") \\\n",
    "        .master(\"spark://192.168.0.112:7077\") \\\n",
    "        .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9001\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Verifica versione Spark\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "    # Verifica configurazione del master\n",
    "    master_config = spark.conf.get(\"spark.master\")\n",
    "    print(f\"Connected to master: {master_config}\")\n",
    "\n",
    "    # Test: crea un DataFrame vuoto\n",
    "    test_df = spark.createDataFrame([], schema=\"id INT, value STRING\")\n",
    "    test_df.show()\n",
    "\n",
    "    print(\"Connection successful, Spark is ready!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect to Spark master.\")\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verifica accesso a HDFS\n",
    "# hdfs_files = spark.read.format(\"text\").load(\"hdfs://localhost:9001/test\")\n",
    "# hdfs_files.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento dei dati da HDFS\n",
    "ratings = spark.read.csv('hdfs://localhost:50010/test/ratings.csv', header=True, inferSchema=True)\n",
    "movies = spark.read.csv('hdfs://localhost:50010/test/movies.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+---------+--------------------+--------------------+\n",
      "|movieId|userId|rating|timestamp|               title|              genres|\n",
      "+-------+------+------+---------+--------------------+--------------------+\n",
      "|      1|     1|   4.0|964982703|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      3|     1|   4.0|964981247|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      6|     1|   4.0|964982224|         Heat (1995)|Action|Crime|Thri...|\n",
      "|     47|     1|   5.0|964983815|Seven (a.k.a. Se7...|    Mystery|Thriller|\n",
      "|     50|     1|   5.0|964982931|Usual Suspects, T...|Crime|Mystery|Thr...|\n",
      "+-------+------+------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge dei dataset ratings e movies\n",
    "user_movie_matrix = ratings.join(movies, on=\"movieId\", how=\"inner\")\n",
    "user_movie_matrix.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Controllo per film comuni valutati da più utenti\n",
    "# common_movies = user_movie_matrix.groupBy(\"title\").count()\n",
    "# common_movies.filter(col(\"count\") > 1).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappatura dei punteggi\n",
    "\n",
    "mapping_score = {\n",
    "        0.5: -1.0,\n",
    "        1: -1.0,\n",
    "        1.5: -0.5,\n",
    "        2: 0.0,\n",
    "        2.5: 0.0,\n",
    "        3: 0.0,\n",
    "        3.5: 0.5,\n",
    "        4: 1.0,\n",
    "        4.5: 1.1,\n",
    "        5: 1.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_score_udf = spark.udf.register(\"map_score\", lambda x: mapping_score.get(x, 0), FloatType())\n",
    "user_movie_matrix = user_movie_matrix.withColumn(\"weight\", map_score_udf(col(\"rating\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione degli edge\n",
    "edges = user_movie_matrix.select(\n",
    "    col(\"userId\").cast(\"string\").alias(\"src\"),\n",
    "    col(\"movieId\").cast(\"string\").alias(\"dst\"),\n",
    "    col(\"weight\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dei vertici\n",
    "\n",
    "user_vertices = user_movie_matrix.select(col(\"userId\").cast(\"string\").alias(\"id\")).distinct().withColumn(\"bipartite\", lit(0))\n",
    "movie_vertices = user_movie_matrix.select(col(\"movieId\").cast(\"string\").alias(\"id\")).distinct().withColumn(\"bipartite\", lit(1))\n",
    "vertices = user_vertices.union(movie_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Creazione del grafo bipartito\n",
    "user_movie_graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proiezione user-user\n",
    "def project_user_user_graph(user_movie_graph):\n",
    "    user_user_edges = user_movie_graph.edges.alias(\"e1\") \\\n",
    "        .join(user_movie_graph.edges.alias(\"e2\"), col(\"e1.dst\") == col(\"e2.dst\")) \\\n",
    "        .select(\n",
    "            col(\"e1.src\").alias(\"src\"),\n",
    "            col(\"e2.src\").alias(\"dst\"),\n",
    "            (col(\"e1.weight\") + col(\"e2.weight\")).alias(\"weight\")\n",
    "        ).filter(col(\"src\") != col(\"dst\"))\n",
    "    return GraphFrame(user_movie_graph.vertices, user_user_edges)\n",
    "\n",
    "user_user_graph = project_user_user_graph(user_movie_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proiezione movie-movie\n",
    "def project_movie_movie_graph(user_movie_graph):\n",
    "    movie_movie_edges = user_movie_graph.edges.alias(\"e1\") \\\n",
    "        .join(user_movie_graph.edges.alias(\"e2\"), col(\"e1.src\") == col(\"e2.src\")) \\\n",
    "        .select(\n",
    "            col(\"e1.dst\").alias(\"src\"),\n",
    "            col(\"e2.dst\").alias(\"dst\"),\n",
    "            (col(\"e1.weight\") + col(\"e2.weight\")).alias(\"weight\")\n",
    "        ).filter(col(\"src\") != col(\"dst\"))\n",
    "    return GraphFrame(user_movie_graph.vertices, movie_movie_edges)\n",
    "\n",
    "movie_movie_graph = project_movie_movie_graph(user_movie_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per calcolare il vettore di preferenze\n",
    "def create_preference_vector(user_id, user_movie_graph):\n",
    "    edges = user_movie_graph.edges.filter(col(\"src\") == user_id).rdd.map(lambda row: (row[\"dst\"], row[\"weight\"])).collect()\n",
    "    tot = sum([weight for _, weight in edges])\n",
    "    if tot > 0:\n",
    "        return {movie: weight / tot for movie, weight in edges}\n",
    "    else:\n",
    "        movies = user_movie_graph.vertices.filter(col(\"bipartite\") == 1).select(\"id\").rdd.map(lambda row: row[0]).collect()\n",
    "        return {movie: 1 / len(movies) for movie in movies}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Page Rank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di predizione\n",
    "\n",
    "def predict_user(user_id, user_movie_graph, movie_movie_graph):\n",
    "    # Crea il vettore di preferenze dell'utente\n",
    "    p_vec = create_preference_vector(user_id, user_movie_graph)\n",
    "    # Film già visti dall'utente\n",
    "    already_seen = [movie for movie, weight in p_vec.items() if weight > 0]\n",
    "    if len(already_seen) == len(p_vec):  # Se ha visto tutti i film, non c'è nulla da predire\n",
    "        return []\n",
    "    \n",
    "    # Calcolo del PageRank sui film\n",
    "    pagerank_results = movie_movie_graph.pageRank(resetProbability=0.95, maxIter=20)\n",
    "    \n",
    "    # Ordina i film in base al PageRank (senza usare .collect() in anticipo)\n",
    "    item_rank = pagerank_results.vertices.select(\"id\", \"pagerank\") \\\n",
    "                                          .filter(~col(\"id\").isin(already_seen)) \\\n",
    "                                          .orderBy(col(\"pagerank\"), ascending=False)\n",
    "    \n",
    "    # Recupera i primi 10 film raccomandati\n",
    "    recommendations = item_rank.limit(10).rdd.map(lambda row: row['id']).collect()\n",
    "    return recommendations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Link Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERA CALCULATE ADAMIC ADAR\n",
    "\n",
    "from pyspark.sql.functions import collect_list\n",
    "def calculate_adamic_adar(graph):\n",
    "    # Trova tutti i vicini per ciascun nodo (utente o film)\n",
    "    neighbors = graph.edges.groupBy(\"src\").agg(collect_list(\"dst\").alias(\"neighbors\"))\n",
    "\n",
    "    # Genera coppie di nodi (film-utente, utente-utente, etc.)\n",
    "    neighbors_df = neighbors.alias(\"n1\").join(\n",
    "        neighbors.alias(\"n2\"), col(\"n1.src\") < col(\"n2.src\")\n",
    "    ).select(\n",
    "        col(\"n1.src\").alias(\"v1\"),\n",
    "        col(\"n2.src\").alias(\"v2\"),\n",
    "        col(\"n1.neighbors\").alias(\"neighbors_v1\"),\n",
    "        col(\"n2.neighbors\").alias(\"neighbors_v2\")\n",
    "    )\n",
    "\n",
    "    # Funzione per calcolare l'indice di Adamic-Adar\n",
    "    def compute_adamic_adar(neighbors_v1, neighbors_v2):\n",
    "        common_neighbors = set(neighbors_v1).intersection(set(neighbors_v2))\n",
    "        if not common_neighbors:\n",
    "            return 0.0\n",
    "        return float(sum(1 / np.log(len(neighbors_v1) + len(neighbors_v2)) for _ in common_neighbors))\n",
    "\n",
    "    # Creazione dell'udf per calcolare l'indice\n",
    "    compute_adamic_adar_udf = udf(compute_adamic_adar, FloatType())\n",
    "\n",
    "    # Calcola l'Adamic-Adar index\n",
    "    adamic_adar_scores = neighbors_df.withColumn(\n",
    "        \"score\", compute_adamic_adar_udf(col(\"neighbors_v1\"), col(\"neighbors_v2\"))\n",
    "    ).filter(col(\"score\") > 0)  # Filtro per evitare punteggi nulli\n",
    "\n",
    "    return adamic_adar_scores.select(\"v1\", \"v2\", \"score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funzione che fa venire più collegamenti\n",
    "\n",
    "# from pyspark.sql.functions import when, col\n",
    "\n",
    "# def calculate_adamic_adar(graph):\n",
    "#     from pyspark.sql.functions import lit\n",
    "\n",
    "#     # Trova tutti i vicini per ciascun nodo (utente o film)\n",
    "#     neighbors = graph.edges.groupBy(\"src\").agg(collect_list(\"dst\").alias(\"neighbors\"))\n",
    "\n",
    "#     # Genera coppie di nodi (film-utente, utente-utente, etc.)\n",
    "#     neighbors_df = neighbors.alias(\"n1\").join(\n",
    "#         neighbors.alias(\"n2\"), col(\"n1.src\") < col(\"n2.src\")\n",
    "#     ).select(\n",
    "#         col(\"n1.src\").alias(\"v1\"),\n",
    "#         col(\"n2.src\").alias(\"v2\"),\n",
    "#         col(\"n1.neighbors\").alias(\"neighbors_v1\"),\n",
    "#         col(\"n2.neighbors\").alias(\"neighbors_v2\")\n",
    "#     )\n",
    "\n",
    "#     # Aggiungi il tipo dei nodi usando il campo bipartite dai vertici\n",
    "#     vertices_with_type = graph.vertices.select(col(\"id\"), col(\"bipartite\"))\n",
    "#     neighbors_with_type = neighbors_df \\\n",
    "#         .join(vertices_with_type.alias(\"v1_type\"), col(\"v1\") == col(\"v1_type.id\")) \\\n",
    "#         .join(vertices_with_type.alias(\"v2_type\"), col(\"v2\") == col(\"v2_type.id\")) \\\n",
    "#         .select(\n",
    "#             col(\"v1\"), col(\"v2\"),\n",
    "#             col(\"neighbors_v1\"), col(\"neighbors_v2\"),\n",
    "#             col(\"v1_type.bipartite\").alias(\"v1_type\"),\n",
    "#             col(\"v2_type.bipartite\").alias(\"v2_type\")\n",
    "#         )\n",
    "\n",
    "#     # Funzione per calcolare l'indice di Adamic-Adar\n",
    "#     def compute_adamic_adar(neighbors_v1, neighbors_v2):\n",
    "#         common_neighbors = set(neighbors_v1).intersection(set(neighbors_v2))\n",
    "#         if not common_neighbors:\n",
    "#             return 0.0\n",
    "#         return float(sum(1 / np.log(len(neighbors_v1) + len(neighbors_v2)) for _ in common_neighbors))\n",
    "\n",
    "#     # Creazione dell'udf per calcolare l'indice\n",
    "#     compute_adamic_adar_udf = udf(compute_adamic_adar, FloatType())\n",
    "\n",
    "#     # Calcola l'Adamic-Adar index\n",
    "#     adamic_adar_scores = neighbors_with_type.withColumn(\n",
    "#         \"score\", compute_adamic_adar_udf(col(\"neighbors_v1\"), col(\"neighbors_v2\"))\n",
    "#     ).filter(col(\"score\") > 0)  # Filtro per evitare punteggi nulli\n",
    "\n",
    "#     # Aggiungi una colonna che indica il tipo di link (questa parte era sbagliata)\n",
    "#     adamic_adar_scores = adamic_adar_scores.withColumn(\n",
    "#         \"link_type\",\n",
    "#         when((col(\"v1_type\") == 0) & (col(\"v2_type\") == 0), \"user-user\")\n",
    "#         .when((col(\"v1_type\") == 1) & (col(\"v2_type\") == 1), \"movie-movie\")\n",
    "#         .otherwise(\"user-movie\")\n",
    "#     )\n",
    "\n",
    "#     return adamic_adar_scores.select(\"v1\", \"v2\", \"score\", \"link_type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram for Adamic-Adar Index\n",
    "def plot_adamic_adar_histogram(adamic_adar_scores):\n",
    "    # Estrai i punteggi in un array\n",
    "    scores = [score[2] for score in adamic_adar_scores]\n",
    "    plt.hist(scores, bins=np.arange(0, max(scores), 0.01), edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Adamic-Adar Index')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Adamic-Adar Index for Predicted Edges')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link prediction and plot\n",
    "# Calcolo dell'indice di Adamic-Adar\n",
    "adamic_adar_scores = calculate_adamic_adar(user_movie_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, count\n",
    "\n",
    "def count_link_types(predicted_edges, vertices):\n",
    "    \"\"\"\n",
    "    Conta le tipologie di link predetti (user-user, movie-movie, user-movie).\n",
    "\n",
    "    :param predicted_edges: DataFrame dei link predetti con colonne 'v1', 'v2', 'score'.\n",
    "    :param vertices: DataFrame dei nodi con colonne 'id' e 'bipartite'.\n",
    "    :return: Un dizionario con i conti per ciascuna tipologia di link.\n",
    "    \"\"\"\n",
    "    # Aggiungi il tipo di nodo (bipartite) per v1 e v2\n",
    "    edges_with_types = predicted_edges \\\n",
    "        .join(vertices.select(col(\"id\").alias(\"v1_id\"), col(\"bipartite\").alias(\"v1_type\")), predicted_edges[\"v1\"] == col(\"v1_id\"), \"left\") \\\n",
    "        .join(vertices.select(col(\"id\").alias(\"v2_id\"), col(\"bipartite\").alias(\"v2_type\")), predicted_edges[\"v2\"] == col(\"v2_id\"), \"left\") \\\n",
    "        .dropDuplicates([\"v1\", \"v2\"])  # Rimuovi duplicati dopo il join\n",
    "\n",
    "    # Classifica i link\n",
    "    link_counts = edges_with_types.withColumn(\n",
    "        \"link_type\",\n",
    "        when((col(\"v1_type\") == 0) & (col(\"v2_type\") == 0), \"user-user\")\n",
    "        .when((col(\"v1_type\") == 1) & (col(\"v2_type\") == 1), \"movie-movie\")\n",
    "        .when((col(\"v1_type\") != col(\"v2_type\")), \"user-movie\")\n",
    "        .otherwise(\"unknown\")\n",
    "    ).groupBy(\"link_type\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "    # Raccogli i risultati in un dizionario\n",
    "    link_counts_dict = {row[\"link_type\"]: row[\"count\"] for row in link_counts.collect()}\n",
    "\n",
    "    # Verifica che il totale corrisponda al numero di link predetti\n",
    "    total_count = sum(link_counts_dict.values())\n",
    "    print(f\"Total predicted edges: {predicted_edges.count()}, Classified edges: {total_count}\")\n",
    "\n",
    "    return link_counts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predicted edges: 164054, Classified edges: 164054\n",
      "Link type counts: {'movie-movie': 121118, 'user-user': 3198, 'user-movie': 39738}\n"
     ]
    }
   ],
   "source": [
    "link_type_counts = count_link_types(adamic_adar_scores, user_movie_graph.vertices)\n",
    "print(\"Link type counts:\", link_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164054\n"
     ]
    }
   ],
   "source": [
    "#print the length of the predicted edges\n",
    "print(len(adamic_adar_scores.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## # PROVA per stampare la tipologia di link trovate con adamic-adar (del tipo \"quanti link user-user ho, e quanti link movie-movie ho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot to find a correct threshold\n",
    "# plot_adamic_adar_histogram(adamic_adar_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predicted_links(graph, predicted_edges, threshold):\n",
    "    # Raccolta dei punteggi di Adamic-Adar in una lista\n",
    "    predicted_edges_list = predicted_edges.collect()\n",
    "\n",
    "    # Filtra gli edge con punteggio superiore alla soglia\n",
    "    new_edges = [(row['v1'], row['v2'], row['score']) for row in predicted_edges_list if row['score'] > threshold]\n",
    "    \n",
    "    # Crea un DataFrame PySpark per i nuovi edge\n",
    "    edges_df = spark.createDataFrame(new_edges, [\"src\", \"dst\", \"weight\"])\n",
    "    \n",
    "    # Unisce i nuovi edge al grafo esistente\n",
    "    extended_graph = GraphFrame(graph.vertices, graph.edges.union(edges_df))\n",
    "    \n",
    "    return extended_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_graph_extended = add_predicted_links(user_movie_graph, adamic_adar_scores, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict movies for a user\n",
    "user =10\n",
    "recommended_movies = predict_user(user, user_movie_graph_extended, movie_movie_graph)\n",
    "print(f\"Recommended movies for user {user}: {recommended_movies[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
